{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9TCQKQ96Z7dT",
        "outputId": "c8cc507a-e80d-4d44-a243-b6117e407604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Store your API key securely and do not share it.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste your API key and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaahith\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260210_061121-i3w1eozp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/saahith/shakespeare-gpt/runs/i3w1eozp' target=\"_blank\">exp_4</a></strong> to <a href='https://wandb.ai/saahith/shakespeare-gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/saahith/shakespeare-gpt' target=\"_blank\">https://wandb.ai/saahith/shakespeare-gpt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/saahith/shakespeare-gpt/runs/i3w1eozp' target=\"_blank\">https://wandb.ai/saahith/shakespeare-gpt/runs/i3w1eozp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter - 0/15684, Train Loss: 3.8816146850585938\n",
            "Iter - 0/15684, Val Loss: 3.65982672279227\n",
            "Iter - 0/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEOU UUS&QP3TT'&I:;LTI GF3K&JZD,F.L\n",
            "3'Y:RB; I AEMF G\n",
            "-\n",
            "LZK UADLPHCEQZO&BY:\n",
            "TVPH\n",
            "ZK3FGFAZDAARJFHB-$BK,;;&T:T:;'FMHXH ;?S I:B-?SF B'NT:.HDARAZ:HXNRJT:RGN:YENN-F.LE H:HCCBKE3BQ:ROQAVB;T'3KS I: B;;?SRG:NT:.U\n",
            "Iter - 100/15684, Train Loss: 2.42573881149292\n",
            "Iter - 200/15684, Train Loss: 2.162304639816284\n",
            "Iter - 300/15684, Train Loss: 1.9593524932861328\n",
            "Iter - 400/15684, Train Loss: 1.8668745756149292\n",
            "Iter - 500/15684, Train Loss: 1.7591204643249512\n",
            "Iter - 500/15684, Val Loss: 1.8169705395679663\n",
            "Iter - 500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO.\n",
            "\n",
            "KING MY BE FOR YESS? THY WEAL ANJULY INTULY WITH,'\n",
            "LADY MY SIRR LOVE!\n",
            "\n",
            "DO THE FALL.\n",
            "\n",
            "SUCIOLUS:\n",
            "THE STORD THAT SHE PLEESS FREATES STRIKE.\n",
            "\n",
            "KENENIUS:\n",
            "FEWELL A WITH THEY HER STRUNES, HEAD WITH.\n",
            "\n",
            "WILL \n",
            "Iter - 600/15684, Train Loss: 1.7168159484863281\n",
            "Iter - 700/15684, Train Loss: 1.6489180326461792\n",
            "Iter - 800/15684, Train Loss: 1.615256667137146\n",
            "Iter - 900/15684, Train Loss: 1.5768250226974487\n",
            "Iter - 1000/15684, Train Loss: 1.5667834281921387\n",
            "Iter - 1000/15684, Val Loss: 1.6255857819742936\n",
            "Iter - 1000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO FOR THE SOUL.\n",
            "\n",
            "POLIXENES:\n",
            "WHEN NOBLE DEGOOD MY PROTESS FACE I TRUCTY THEM,\n",
            "NEIR AT ALL SHAME THOU DID BY DID,\n",
            "THAT HIS DAY, INFORCE HAD SAY BE THE FORTUTE AND AN TO\n",
            "AND LORD'S A BOY DEED OF THE FEEL \n",
            "Iter - 1100/15684, Train Loss: 1.5791096687316895\n",
            "Iter - 1200/15684, Train Loss: 1.4834693670272827\n",
            "Iter - 1300/15684, Train Loss: 1.4816508293151855\n",
            "Iter - 1400/15684, Train Loss: 1.493112564086914\n",
            "Iter - 1500/15684, Train Loss: 1.4712190628051758\n",
            "Iter - 1500/15684, Val Loss: 1.52891306946948\n",
            "Iter - 1500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO:\n",
            "THINK IS, AND, TURN AND LOST THOU BRIST\n",
            "BUT HIS SWEET AND WENCH OPINION YOUR SHALL NOT THE FACE?\n",
            "\n",
            "FRIUS:\n",
            "WHAT IS IT IS SUCH IN FASH.\n",
            "THE GRACIOUS MAYOR ARE SUN HAD CAN MAY, AS MY FACE;\n",
            "FAIRS THOU TR\n",
            "Iter - 1600/15684, Train Loss: 1.447775959968567\n",
            "Iter - 1700/15684, Train Loss: 1.466078758239746\n",
            "Iter - 1800/15684, Train Loss: 1.4884741306304932\n",
            "Iter - 1900/15684, Train Loss: 1.4213508367538452\n",
            "Iter - 2000/15684, Train Loss: 1.4092869758605957\n",
            "Iter - 2000/15684, Val Loss: 1.4813867223430164\n",
            "Iter - 2000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO?\n",
            "\n",
            "FIRST WARWICER:\n",
            "THE WE HAVE PEOPLE TAKE MAY SHOUTS, AND HAVE NEWS?\n",
            "\n",
            "ROMEO:\n",
            "AS NOT A LORD, NO HE MAY WRONG'?\n",
            "NOW YOU.\n",
            "\n",
            "MARIANA:\n",
            "AH, BUT IF YOU DO YOU ARE, MAY LADY.\n",
            "\n",
            "SICINIUS:\n",
            "FOR THEY IS IN MY TRUE\n",
            "Iter - 2100/15684, Train Loss: 1.4077866077423096\n",
            "Iter - 2200/15684, Train Loss: 1.394330620765686\n",
            "Iter - 2300/15684, Train Loss: 1.3972803354263306\n",
            "Iter - 2400/15684, Train Loss: 1.433424949645996\n",
            "Iter - 2500/15684, Train Loss: 1.4120672941207886\n",
            "Iter - 2500/15684, Val Loss: 1.4506090050997975\n",
            "Iter - 2500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO WEEP; BUT A MINE ALREADY! THERE\n",
            "IS BETTER MINE HER STANDS.\n",
            "\n",
            "ROMEO:\n",
            "I THANK THERE HERE, I WILL SWEARING-WORDS,\n",
            "WHERE I LIVE IT. SIGHTS, SOMETHINK MY BABES!\n",
            "\n",
            "MARCIUS:\n",
            "HE HATH WIND SAINT YOUR THEIR GRON\n",
            "Iter - 2600/15684, Train Loss: 1.3750661611557007\n",
            "Iter - 2700/15684, Train Loss: 1.3412704467773438\n",
            "Iter - 2800/15684, Train Loss: 1.3417844772338867\n",
            "Iter - 2900/15684, Train Loss: 1.352973222732544\n",
            "Iter - 3000/15684, Train Loss: 1.3287101984024048\n",
            "Iter - 3000/15684, Val Loss: 1.4283449318491082\n",
            "Iter - 3000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST.\n",
            "\n",
            "DUKE OF YORK:\n",
            "I WAS THE DUKE OF THE GODS CELL'D TO ANY SWEARS.\n",
            "\n",
            "BENVOLIO:\n",
            "WHAT HAS BUT THEY TO RENOWN IN THEM.\n",
            "NOW NEVER TALK OF AS FALL OF FRIARS,\n",
            "HIS FACE THE GREATER SCONDLY THE FAIL OF FACT,\n",
            "Iter - 3100/15684, Train Loss: 1.3242751359939575\n",
            "Iter - 3200/15684, Train Loss: 1.3321888446807861\n",
            "Iter - 3300/15684, Train Loss: 1.3219280242919922\n",
            "Iter - 3400/15684, Train Loss: 1.3007652759552002\n",
            "Iter - 3500/15684, Train Loss: 1.2999002933502197\n",
            "Iter - 3500/15684, Val Loss: 1.412423250784783\n",
            "Iter - 3500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO,\n",
            "I HOPE IN HIS FAIR: SCOME WE HAD, LIVES,\n",
            "THUS AFTERNATE SERVICE, AS SEE WHERE.\n",
            "\n",
            "SICINIUS:\n",
            "AND IF WE SAW HIM\n",
            "WHERE COMES YOU OUT, TAKE UP TO FRIENDS\n",
            "THAT HAVE YOU SHALL BE SO INTO FRIAR?\n",
            "SUPPOSE HIM,\n",
            "Iter - 3600/15684, Train Loss: 1.2973432540893555\n",
            "Iter - 3700/15684, Train Loss: 1.3142043352127075\n",
            "Iter - 3800/15684, Train Loss: 1.331674337387085\n",
            "Iter - 3900/15684, Train Loss: 1.3081367015838623\n",
            "Iter - 4000/15684, Train Loss: 1.3159048557281494\n",
            "Iter - 4000/15684, Val Loss: 1.3971631935517441\n",
            "Iter - 4000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO SINCE\n",
            "WE MAY THOUGHT HE HATH SOMETHING: I'LL DAY;\n",
            "WHICH DOTH IS SOON.\n",
            "\n",
            "NURSE:\n",
            "AND HE THAT\n",
            "YOU THOUGHT I THE COMMON OF HITHER;\n",
            "AND MAKES NOT FROM MY BOOKS WITH SOME SPELL,\n",
            "OR IF THE FACE MORE WILL BE \n",
            "Iter - 4100/15684, Train Loss: 1.3296709060668945\n",
            "Iter - 4200/15684, Train Loss: 1.3202732801437378\n",
            "Iter - 4300/15684, Train Loss: 1.2996515035629272\n",
            "Iter - 4400/15684, Train Loss: 1.284346342086792\n",
            "Iter - 4500/15684, Train Loss: 1.2924542427062988\n",
            "Iter - 4500/15684, Val Loss: 1.3908851979575094\n",
            "Iter - 4500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST BE WORD\n",
            "AND MORE STANDS, BOLINGBROKE, WITH STIR THEY MUST\n",
            "I HID FOUR, HAD AN INFERRING THE LIGHT.\n",
            "WHAT'S THOU? WHY? WHY, FOR THIS HOLY FATHER'S PROVE.\n",
            "\n",
            "FIRST CITIZEN:\n",
            "WORTHY! THE GREAT SIR, BLESSI\n",
            "Iter - 4600/15684, Train Loss: 1.291651725769043\n",
            "Iter - 4700/15684, Train Loss: 1.2513914108276367\n",
            "Iter - 4800/15684, Train Loss: 1.263594627380371\n",
            "Iter - 4900/15684, Train Loss: 1.2904446125030518\n",
            "Iter - 5000/15684, Train Loss: 1.2533972263336182\n",
            "Iter - 5000/15684, Val Loss: 1.3897176113189318\n",
            "Iter - 5000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST!\n",
            "A CAPULET, WARWICK, WHAT'S AN ENCROWN,\n",
            "IS IT BY ADVISED TO THEIR HATEFUL SHAME:\n",
            "STRIKE ME DOTH NEVER DIETH IN THY LOVE,\n",
            "BEFORE THE SUN AND LOVE THE TIME OWN FIGHT.\n",
            "\n",
            "FIRST MURDERER:\n",
            "SUBJECT OF THE\n",
            "Iter - 5100/15684, Train Loss: 1.2663191556930542\n",
            "Iter - 5200/15684, Train Loss: 1.232517957687378\n",
            "Iter - 5300/15684, Train Loss: 1.2558964490890503\n",
            "Iter - 5400/15684, Train Loss: 1.2704038619995117\n",
            "Iter - 5500/15684, Train Loss: 1.272119402885437\n",
            "Iter - 5500/15684, Val Loss: 1.3781070050880802\n",
            "Iter - 5500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "AY, BY MY GENTLE LORD, FOR THEY HAVE WIND,\n",
            "NOR WILL, AND LIVE THY LAMB FROM THEE,\n",
            "NOT SIGNIOUS COWARD IN A WANTON SELF. I HAVE PERFORM.\n",
            "\n",
            "POLIXENES:\n",
            "AND WE'LL CONCERN YOU. ART YO\n",
            "Iter - 5600/15684, Train Loss: 1.242558479309082\n",
            "Iter - 5700/15684, Train Loss: 1.255683422088623\n",
            "Iter - 5800/15684, Train Loss: 1.2499339580535889\n",
            "Iter - 5900/15684, Train Loss: 1.241999626159668\n",
            "Iter - 6000/15684, Train Loss: 1.2111749649047852\n",
            "Iter - 6000/15684, Val Loss: 1.3829646701502596\n",
            "Iter - 6000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST TO DELIVER'D\n",
            "HIM: HOW SHALL THIS BE ACCESS AND HOW I THE DEAD\n",
            "AND BOTH BY PATIENCE OF WHAT YOU HAVE NO DUTY\n",
            "SO WILL WHEN HE HAS A STIRRING DISPOSITION,\n",
            "THE SUN THIS NIGHT, OR WOILE ONCE MADE,\n",
            "A HI\n",
            "Iter - 6100/15684, Train Loss: 1.2052702903747559\n",
            "Iter - 6200/15684, Train Loss: 1.240725040435791\n",
            "Iter - 6300/15684, Train Loss: 1.2068849802017212\n",
            "Iter - 6400/15684, Train Loss: 1.2111725807189941\n",
            "Iter - 6500/15684, Train Loss: 1.214520812034607\n",
            "Iter - 6500/15684, Val Loss: 1.3746063099836767\n",
            "Iter - 6500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST!\n",
            "TRAITOR, THAT DO NOT TAKE HIM LEAVING DOUBTFUL\n",
            "MAKES ONE FATHER THAN SHE HATH NOT, AND I AM\n",
            "TO-MORROW; HE IS MY CONFESSED SUCH A DRUM,\n",
            "IS THEN, SOFTER OF A MAN IN PEACE A STRAIGHT,\n",
            "OR SPEEDY PENS\n",
            "Iter - 6600/15684, Train Loss: 1.22528874874115\n",
            "Iter - 6700/15684, Train Loss: 1.2058191299438477\n",
            "Iter - 6800/15684, Train Loss: 1.1900324821472168\n",
            "Iter - 6900/15684, Train Loss: 1.2182812690734863\n",
            "Iter - 7000/15684, Train Loss: 1.2051787376403809\n",
            "Iter - 7000/15684, Val Loss: 1.3734635745550596\n",
            "Iter - 7000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST\n",
            "TROD MORE DANGEROUS POINT: IT IS AN RESOLVE\n",
            "TO HOLLOW HIS SECOND CONQUEST ON HIM.\n",
            "IF SO BE A THOUSAND SAD IS TRUE, AND HE'S NOT\n",
            "FROM THE FIERY SCAPE. O HORROR, FOR THAT YOU THE\n",
            "TALE\n",
            "AND COLD THE P\n",
            "Iter - 7100/15684, Train Loss: 1.2252447605133057\n",
            "Iter - 7200/15684, Train Loss: 1.1904757022857666\n",
            "Iter - 7300/15684, Train Loss: 1.2509709596633911\n",
            "Iter - 7400/15684, Train Loss: 1.1838383674621582\n",
            "Iter - 7500/15684, Train Loss: 1.232419490814209\n",
            "Iter - 7500/15684, Val Loss: 1.3787409286889283\n",
            "Iter - 7500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST.\n",
            "\n",
            "AUTOLYCUS:\n",
            "TWENTY! GO THEE, WHAT LOOK YOU OF?\n",
            "\n",
            "LUCIO:\n",
            "THIS IS A GOOD FORTUNE WAS THE PRINCE OF THEIR\n",
            "PIERCE LAWFUL TITLE IN THIS FREE AND PLACE.\n",
            "\n",
            "SERVANT:\n",
            "I HAVE NOT MAD BEEN.\n",
            "THOU SEND TO SEE T\n",
            "Iter - 7600/15684, Train Loss: 1.1975713968276978\n",
            "Iter - 7700/15684, Train Loss: 1.2109663486480713\n",
            "Iter - 7800/15684, Train Loss: 1.200555682182312\n",
            "Iter - 7900/15684, Train Loss: 1.1966371536254883\n",
            "Iter - 8000/15684, Train Loss: 1.1820812225341797\n",
            "Iter - 8000/15684, Val Loss: 1.36941998114781\n",
            "Iter - 8000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO SHALT REVENGE?\n",
            "\n",
            "LUCIO:\n",
            "NO, IF I KNOW, WITH THAT SHE COMELY; BE MUCH NOT,\n",
            "BOY, GO WOULD FRIGHT, NOT LIKE MY FARTHER'S HONOUR!\n",
            "YOU THRIVE SHALL BE SENSED: AND I'LL REMEMBER\n",
            "IN THEIR PERSONS OF HIS MIST\n",
            "Iter - 8100/15684, Train Loss: 1.1973719596862793\n",
            "Iter - 8200/15684, Train Loss: 1.1649961471557617\n",
            "Iter - 8300/15684, Train Loss: 1.1758508682250977\n",
            "Iter - 8400/15684, Train Loss: 1.1752946376800537\n",
            "Iter - 8500/15684, Train Loss: 1.1780363321304321\n",
            "Iter - 8500/15684, Val Loss: 1.3712581094968195\n",
            "Iter - 8500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST,\n",
            "BEAR THE COMMON STRANGE, FOR NEVER WORTH THESE\n",
            "WOFUL CAUSE!\n",
            "\n",
            "SECOND LORD:\n",
            "NAY, THEN! FAWNS IN THE DEMAND IS AS THE GATE\n",
            "SHORES BUT THE HEARTS, SAVE STORMY TRIBUNES.\n",
            "\n",
            "LUCIO:\n",
            "O, WILT THOU CHEQUE TH\n",
            "Iter - 8600/15684, Train Loss: 1.1577190160751343\n",
            "Iter - 8700/15684, Train Loss: 1.148116111755371\n",
            "Iter - 8800/15684, Train Loss: 1.1554145812988281\n",
            "Iter - 8900/15684, Train Loss: 1.1939210891723633\n",
            "Iter - 9000/15684, Train Loss: 1.1531198024749756\n",
            "Iter - 9000/15684, Val Loss: 1.3768888153006131\n",
            "Iter - 9000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO?\n",
            "SOUND THIS IS A DIRECTION LAST?\n",
            "\n",
            "MONTAGUE:\n",
            "TO TAKE YOUR HONOUR FACTIONS TRUE-WALLOW'D.\n",
            "\n",
            "ALL:\n",
            "NO, BY THE WHICH, MY LOVE, OR I DREAM.\n",
            "\n",
            "ROMEO:\n",
            "ALL DULL ASSEMBLY SHOW THY NAME AND LAMENT\n",
            "SHOWS IN ME, AN\n",
            "Iter - 9100/15684, Train Loss: 1.1650019884109497\n",
            "Iter - 9200/15684, Train Loss: 1.1857558488845825\n",
            "Iter - 9300/15684, Train Loss: 1.1195745468139648\n",
            "Iter - 9400/15684, Train Loss: 1.1405165195465088\n",
            "Iter - 9500/15684, Train Loss: 1.165703296661377\n",
            "Iter - 9500/15684, Val Loss: 1.3785265708876646\n",
            "Iter - 9500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST, AND\n",
            "YET THOU ART NOT STIFL'D YOUR MAJESTY.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "AND SWEET SAID WAS EASY, FAME THY LIFE, ONE WORD,\n",
            "WHOSE FOLK IS NOT FITTER, LOOKING OF THE DUTY'S BLOOD.\n",
            "\n",
            "KING RICHARD II:\n",
            "BY MY CAIUS \n",
            "Iter - 9600/15684, Train Loss: 1.1593974828720093\n",
            "Iter - 9700/15684, Train Loss: 1.1556792259216309\n",
            "Iter - 9800/15684, Train Loss: 1.1212513446807861\n",
            "Iter - 9900/15684, Train Loss: 1.121881127357483\n",
            "Iter - 10000/15684, Train Loss: 1.164984941482544\n",
            "Iter - 10000/15684, Val Loss: 1.3773653090794118\n",
            "Iter - 10000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO?\n",
            "\n",
            "NURSE:\n",
            "O, MY SON! I'LL KEEP IT WAS OURS!\n",
            "\n",
            "CLOWN:\n",
            "DOST THOU DISPATCH; 'TIS THREE BUT NEITHER: HEAR ME,\n",
            "AS IF I LAY AS BEASTLY BEG, BEST FOR THE EARS,\n",
            "NOR I, OUR WAY, WITHOUT DAUGHTER, AND DISCOURSER\n",
            "Iter - 10100/15684, Train Loss: 1.1330312490463257\n",
            "Iter - 10200/15684, Train Loss: 1.1155009269714355\n",
            "Iter - 10300/15684, Train Loss: 1.1283174753189087\n",
            "Iter - 10400/15684, Train Loss: 1.1467857360839844\n",
            "Iter - 10500/15684, Train Loss: 1.1139416694641113\n",
            "Iter - 10500/15684, Val Loss: 1.3787915927228214\n",
            "Iter - 10500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST,\n",
            "AND WHEN A STARS AND RODDEN BIARDS DRAW WITH SHELTERS,\n",
            "TO HIS PRECIOUS SIR, MY POOR CAPTAIN,\n",
            "HAVING HIS SOUR SO DETERMINED TO MY HEART;\n",
            "AND BOWS HIS CENSURES BY ARMS WITH BLEEDING-HANDS.\n",
            "IF ALL T\n",
            "Iter - 10600/15684, Train Loss: 1.1421167850494385\n",
            "Iter - 10700/15684, Train Loss: 1.110493540763855\n",
            "Iter - 10800/15684, Train Loss: 1.110268235206604\n",
            "Iter - 10900/15684, Train Loss: 1.0865414142608643\n",
            "Iter - 11000/15684, Train Loss: 1.1018534898757935\n",
            "Iter - 11000/15684, Val Loss: 1.3816484746763604\n",
            "Iter - 11000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO, WILT TAKE THEE THEE\n",
            "FOR THEY ARE MADE TO A TORCH-PACE, THEN HE SHALL\n",
            "POST TO YOU BANISHMENT!\n",
            "\n",
            "PRINCE EDWARD:\n",
            "WHAT HIGH YOU THAT? I BROUGHT IT WAS THIS,\n",
            "THAT FOR THY SEATING MOUTH NOT, THOU HAST THE \n",
            "Iter - 11100/15684, Train Loss: 1.0919313430786133\n",
            "Iter - 11200/15684, Train Loss: 1.0768429040908813\n",
            "Iter - 11300/15684, Train Loss: 1.131972312927246\n",
            "Iter - 11400/15684, Train Loss: 1.1089781522750854\n",
            "Iter - 11500/15684, Train Loss: 1.1098146438598633\n",
            "Iter - 11500/15684, Val Loss: 1.3895227591358827\n",
            "Iter - 11500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO, TRUST IT IN THEE.\n",
            "O, AND PRAY YOU, BY THE HAND OF THE WIT;\n",
            "IF IF YOU WAS FORM US TO YOUR COMFORT, I BESEECH YOU,\n",
            "IS TOO FULL FORTUNE AND WELL BE STRONG.\n",
            "\n",
            "BUCKINGHAM:\n",
            "GO, MADAM, FEAR ME NOT, TEARS TO\n",
            "Iter - 11600/15684, Train Loss: 1.0979382991790771\n",
            "Iter - 11700/15684, Train Loss: 1.086861491203308\n",
            "Iter - 11800/15684, Train Loss: 1.0782594680786133\n",
            "Iter - 11900/15684, Train Loss: 1.1195943355560303\n",
            "Iter - 12000/15684, Train Loss: 1.0882775783538818\n",
            "Iter - 12000/15684, Val Loss: 1.3975022198643758\n",
            "Iter - 12000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST?\n",
            "\n",
            "PROVOST:\n",
            "AND THEREFORE BE STILL.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "NO, NOR A MAN:\n",
            "WHAT BUT THIS IS SO LEVEL TO ENDURE HIM,\n",
            "THEY MAY BE MORE, BUT LET HIM PLEAD FOR HIM;\n",
            "AND, SHE DOTH KNOW WITH THEE, WIFE, IN THE \n",
            "Iter - 12100/15684, Train Loss: 1.1263599395751953\n",
            "Iter - 12200/15684, Train Loss: 1.0922949314117432\n",
            "Iter - 12300/15684, Train Loss: 1.0886647701263428\n",
            "Iter - 12400/15684, Train Loss: 1.1257389783859253\n",
            "Iter - 12500/15684, Train Loss: 1.0698626041412354\n",
            "Iter - 12500/15684, Val Loss: 1.3984488183734198\n",
            "Iter - 12500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST,\n",
            "SCARCELY SCANDAL TO ANY SWEET BOY THAT;\n",
            "FOR WHEN THEIR LOSS CAN NOTHING LET THEM FEAR,\n",
            "SOME WORD WILL NOT BE RULED APPEALED;\n",
            "THE TONGUES OF HER BEHALFS, IT IS, TO LOSE HIS\n",
            "CHILDREN FROM HIS BREAS\n",
            "Iter - 12600/15684, Train Loss: 1.093824863433838\n",
            "Iter - 12700/15684, Train Loss: 1.0863158702850342\n",
            "Iter - 12800/15684, Train Loss: 1.0791399478912354\n",
            "Iter - 12900/15684, Train Loss: 1.0956730842590332\n",
            "Iter - 13000/15684, Train Loss: 1.068342685699463\n",
            "Iter - 13000/15684, Val Loss: 1.4050235575192203\n",
            "Iter - 13000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO SLAIN?\n",
            "\n",
            "SECOND MESSENGER:\n",
            "AND HOW THEN SAME THE KINGDOM OF THY HEAD:\n",
            "THOU SHALT SHE SPEAK TO SHOW FAR FLOURISH FROM HER\n",
            "FACES WHERE ATTAGENET. I MUST CALL HIM ALL THE BEST\n",
            "AND CAST THEIR FACES TO PRI\n",
            "Iter - 13100/15684, Train Loss: 1.0603125095367432\n",
            "Iter - 13200/15684, Train Loss: 1.0610053539276123\n",
            "Iter - 13300/15684, Train Loss: 1.0876423120498657\n",
            "Iter - 13400/15684, Train Loss: 1.095217227935791\n",
            "Iter - 13500/15684, Train Loss: 1.067713975906372\n",
            "Iter - 13500/15684, Val Loss: 1.4062620474198393\n",
            "Iter - 13500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO?\n",
            "\n",
            "THIRD SERVINGMAN:\n",
            "WHICH TELLS US THE FRIAR AND GROUND\n",
            "SUBSTANTIAL; FOR HER HEART THOU, FOR MY HEART,\n",
            "WILT THOU MAKE HOST MAJESTY TO HIM. AND HOW COMEST\n",
            "I'LL BE IMPRISON'D TO THE KING; INDEED, AS WE\n",
            "Iter - 13600/15684, Train Loss: 1.0640796422958374\n",
            "Iter - 13700/15684, Train Loss: 1.059581995010376\n",
            "Iter - 13800/15684, Train Loss: 1.0649752616882324\n",
            "Iter - 13900/15684, Train Loss: 1.0710300207138062\n",
            "Iter - 14000/15684, Train Loss: 1.064134955406189\n",
            "Iter - 14000/15684, Val Loss: 1.4163958504322152\n",
            "Iter - 14000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO REQUIRE THE EASTS,\n",
            "WHOSE BREAK THOUSAND LORDS SELT THOU, LIKE TO CRY\n",
            "THIS DAY AS I WAS A GILLYVOR HARD THAT HAD DETERM'D\n",
            "A DOZEN DELIGHT AS I CAN TELL THEE OF THY HEAD\n",
            "AND THUS HAVE STRETCH'D THY FOR\n",
            "Iter - 14100/15684, Train Loss: 1.0548219680786133\n",
            "Iter - 14200/15684, Train Loss: 1.0519521236419678\n",
            "Iter - 14300/15684, Train Loss: 1.062357783317566\n",
            "Iter - 14400/15684, Train Loss: 1.0468486547470093\n",
            "Iter - 14500/15684, Train Loss: 1.0656487941741943\n",
            "Iter - 14500/15684, Val Loss: 1.4220918533711335\n",
            "Iter - 14500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO?\n",
            "\n",
            "SLY:\n",
            "I KNOW THE KING; LET US BE MUCH SO SINGLE\n",
            "AT ALL THOU CRY ME FROM THY HUMOURS THAT THY LOSS\n",
            "OF THAT THOU HAST SURE OUR SISTER, THAN SO TOO,\n",
            "THAT THOU SHALT HELP OUT FOR A CAUSE OF LINE,\n",
            "OF APE\n",
            "Iter - 14600/15684, Train Loss: 1.0593032836914062\n",
            "Iter - 14700/15684, Train Loss: 1.0559701919555664\n",
            "Iter - 14800/15684, Train Loss: 1.0621898174285889\n",
            "Iter - 14900/15684, Train Loss: 1.0615272521972656\n",
            "Iter - 15000/15684, Train Loss: 1.0261852741241455\n",
            "Iter - 15000/15684, Val Loss: 1.4211637463358489\n",
            "Iter - 15000/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST ON MY DAUGHTER'S HEAD!\n",
            "WERE I A COUNTED SAFELY FIND, A RAVENSPURGH,\n",
            "WERE ALL OBDURATE TO AUGHT OF MARCH AMONGST THE DUKE.\n",
            "\n",
            "KING EDWARD IV:\n",
            "SO PRITHEE, WARWICK, LADS, AND TELL THEE BURN;\n",
            "STAY SERVE\n",
            "Iter - 15100/15684, Train Loss: 1.037421703338623\n",
            "Iter - 15200/15684, Train Loss: 1.0507454872131348\n",
            "Iter - 15300/15684, Train Loss: 1.0086166858673096\n",
            "Iter - 15400/15684, Train Loss: 1.0531951189041138\n",
            "Iter - 15500/15684, Train Loss: 1.028062105178833\n",
            "Iter - 15500/15684, Val Loss: 1.4313391253654373\n",
            "Iter - 15500/15684, Sample Generation:\n",
            "WHEREFORE ARE THOU ROMEO'ST,\n",
            "EVEN HATH NO MORE FROM THE SOULS. NOW, PROVOST,\n",
            "THESE WEEPING HEARTS WITH PRESENT, HELL FOR THEM.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "OFF CAIUS, MY GRACIOUS LORD, MAKE PARTICULAR\n",
            "CUT OFF WITH THEM; HEAVY AND DAUGHT\n",
            "Iter - 15600/15684, Train Loss: 1.016969919204712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>iter</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/loss</td><td>█▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>iter</td><td>15600</td></tr><tr><td>sample/text</td><td>WHEREFORE ARE THOU R...</td></tr><tr><td>train/loss</td><td>1.01697</td></tr><tr><td>val/loss</td><td>1.43134</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">exp_4</strong> at: <a href='https://wandb.ai/saahith/shakespeare-gpt/runs/i3w1eozp' target=\"_blank\">https://wandb.ai/saahith/shakespeare-gpt/runs/i3w1eozp</a><br> View project at: <a href='https://wandb.ai/saahith/shakespeare-gpt' target=\"_blank\">https://wandb.ai/saahith/shakespeare-gpt</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260210_061121-i3w1eozp/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "import random\n",
        "\n",
        "BLOCK_SIZE = 128\n",
        "N_EMBD = 256\n",
        "N_LAYERS = 8\n",
        "N_HEAD = 8\n",
        "DROPOUT_PROB = 0.2\n",
        "USE_BIAS = False\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# File-path helpers for local vs. Google Drive execution\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules or os.environ.get(\"COLAB_RELEASE_TAG\") is not None\n",
        "\n",
        "try:\n",
        "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
        "except NameError:\n",
        "    # __file__ is not defined inside interactive/Colab notebooks\n",
        "    SCRIPT_DIR = Path.cwd()\n",
        "\n",
        "PROJECT_ROOT = SCRIPT_DIR.parent\n",
        "\n",
        "\n",
        "def ensure_drive_mounted() -> Optional[Path]:\n",
        "    \"\"\"Mount Google Drive when running inside Colab and return the MyDrive path.\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        return None\n",
        "\n",
        "    drive_root = Path(\"/content/drive\")\n",
        "    mydrive = drive_root / \"MyDrive\"\n",
        "    if mydrive.exists():\n",
        "        return mydrive\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive as gdrive  # type: ignore\n",
        "    except ModuleNotFoundError as exc:  # pragma: no cover\n",
        "        raise RuntimeError(\n",
        "            \"google.colab module not available; cannot mount Google Drive.\"\n",
        "        ) from exc\n",
        "\n",
        "    gdrive.mount(str(drive_root))\n",
        "    return mydrive\n",
        "\n",
        "\n",
        "MYDRIVE_ROOT = ensure_drive_mounted()\n",
        "\n",
        "\n",
        "def resolve_data_file(filename: str) -> Path:\n",
        "    \"\"\"\n",
        "    Return the first existing path to `filename` among known data locations.\n",
        "\n",
        "    Order:\n",
        "    1. Original relative path (current working directory based)\n",
        "    2. Repository structure relative to this script\n",
        "    3. Google Drive repo mirror (if running inside Colab)\n",
        "    \"\"\"\n",
        "    candidates = [\n",
        "        Path(\"..\") / \"data\" / filename,\n",
        "        PROJECT_ROOT / \"data\" / filename,\n",
        "    ]\n",
        "\n",
        "    if MYDRIVE_ROOT:\n",
        "        candidates.extend(\n",
        "            [\n",
        "                MYDRIVE_ROOT / \"ai-laboratory\" / \"shakespeare-gpt\" / \"data\" / filename,\n",
        "                MYDRIVE_ROOT / \"shakespeare-gpt\" / \"data\" / filename,\n",
        "                MYDRIVE_ROOT / \"data\" / filename,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    for candidate in candidates:\n",
        "        candidate = candidate.expanduser().resolve()\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not locate {filename}. Checked: {', '.join(str(p) for p in candidates)}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def read_data_file(filename: str) -> str:\n",
        "    data_path = resolve_data_file(filename)\n",
        "    with data_path.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "        return handle.read()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "def get_best_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device('mps')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "\n",
        "device = get_best_device()\n",
        "\n",
        "\n",
        "# read and tokenize the data at the character level\n",
        "train_text = read_data_file(\"train.txt\")\n",
        "val_text = read_data_file(\"val.txt\")\n",
        "test_text = read_data_file(\"test.txt\")\n",
        "\n",
        "\n",
        "# note: this should be deterministic\n",
        "combined_text = train_text + val_text + test_text\n",
        "unique_chars = sorted(list(set(combined_text)))\n",
        "VOCAB_SIZE = len(unique_chars)\n",
        "\n",
        "\n",
        "stoi = {c: i for i,c in enumerate(unique_chars)}\n",
        "itos = {i: c for i,c in enumerate(unique_chars)}\n",
        "\n",
        "\n",
        "train_tokens = [stoi[c] for c in train_text]\n",
        "val_tokens = [stoi[c] for c in val_text]\n",
        "test_tokens = [stoi[c] for c in test_text]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################################################\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        # fetch the tokens\n",
        "        if split == 'train':\n",
        "            data = train_tokens\n",
        "        elif split == 'val':\n",
        "            data = val_tokens\n",
        "        elif split == 'test':\n",
        "            data = test_tokens\n",
        "        else:\n",
        "            raise ValueError(f\"split argument must be one of: [train, val, test], you gave {split}\")\n",
        "\n",
        "        self.data = torch.tensor(data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx: idx + BLOCK_SIZE]\n",
        "        y = self.data[idx+1: idx + 1 + BLOCK_SIZE]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - BLOCK_SIZE - 1\n",
        "\n",
        "############################################################################\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super().__init__()\n",
        "        self.W_up = nn.Linear(N_EMBD, 4 * N_EMBD, bias=USE_BIAS)\n",
        "        self.act = nn.SiLU()\n",
        "        self.W_gate = nn.Linear(N_EMBD, 4 * N_EMBD, bias=USE_BIAS)\n",
        "\n",
        "        self.W_down = nn.Linear(4 * N_EMBD, N_EMBD, bias=USE_BIAS)\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_PROB)\n",
        "\n",
        "    def forward(self, x):\n",
        "        prod = self.act(self.W_gate(x)) * self.W_up(x)\n",
        "        out = self.W_down(self.dropout(prod))\n",
        "        return out\n",
        "\n",
        "############################################################################\n",
        "def apply_rotary_emb(x, cos, sin):\n",
        "    assert x.ndim == 4 # (B, T, n_h, h_dim)\n",
        "    d = x.size(3) // 2\n",
        "    x1, x2 = x[..., d:], x[..., :d] # ()\n",
        "    y1 = x1 * cos + x2 * sin\n",
        "\n",
        "    # print(f\"x1.shape: {x1.shape}, cos.shape: {cos.shape}\")\n",
        "\n",
        "    y2 = x1 * (-sin) + x2 * cos\n",
        "    y = torch.cat([y1, y2], dim=3)\n",
        "    y = y.to(x.dtype)\n",
        "    return y\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c_attn = nn.Linear(N_EMBD, 3 * N_EMBD, bias=USE_BIAS)\n",
        "        self.c_proj = nn.Linear(N_EMBD, N_EMBD, bias=USE_BIAS)\n",
        "\n",
        "        # (1, 1, BLOCK_SIZE, BLO)\n",
        "        self.register_buffer(\"att_mask\", torch.triu(torch.ones(BLOCK_SIZE, BLOCK_SIZE) * -float('inf'), diagonal=1).view(1, 1, BLOCK_SIZE, BLOCK_SIZE))\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(p=DROPOUT_PROB) # applied before c_proj, after softmax\n",
        "        self.resid_dropout = nn.Dropout(p=DROPOUT_PROB) # applied right before returning\n",
        "\n",
        "\n",
        "    def forward(self, x, cos, sin):\n",
        "        B, T, C = x.size() # (B,T,C)\n",
        "        q, k, v = self.c_attn(x).split(split_size = N_EMBD, dim=2) # (B, T, C) --> (B, T, 3C) --> (B, T, C), (B, T, C) , (B, T, C)\n",
        "\n",
        "        q = q.view(B, T, N_HEAD, -1) # (B, T, C) --> (B, T, N_H, H_D)\n",
        "        k = k.view(B, T, N_HEAD, -1) # (B, T, C) --> (B, T, N_H, H_D)\n",
        "        v = v.view(B, T, N_HEAD, -1) # (B, T, C) --> (B, T, N_H, H_D)\n",
        "\n",
        "        q = apply_rotary_emb(q, cos, sin)\n",
        "        k = apply_rotary_emb(k, cos, sin)\n",
        "\n",
        "        q = q.transpose(1,2)\n",
        "        k = k.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        H_D = q.size(-1)\n",
        "        # self attention!!!\n",
        "        att_scores = q @ k.transpose(-1, -2) / (H_D ** 0.5) # (B, N_H, T, H_D) @ (B, N_H, H_D, T) --> (B, N_H, T, T)\n",
        "        att_scores = att_scores + self.att_mask[:, :, :T, :T] # (B, N_H, T, T)\n",
        "        att_scores = F.softmax(att_scores, dim=-1)\n",
        "        att_scores = self.attn_dropout(att_scores)\n",
        "\n",
        "        out = att_scores @ v # (B, N_H, T, T) @ (B, N_H, T, H_D) --> (B, N_H, T, H_D)\n",
        "        out = out.transpose(1,2).contiguous().view(B,T,-1) # (B, T, N_H, H_D) --> (B, T, C)\n",
        "\n",
        "        out = self.c_proj(out)\n",
        "        out = self.resid_dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "############################################################################\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, ):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(normalized_shape = (N_EMBD,))\n",
        "    self.sa = SelfAttention()\n",
        "    self.ln_2 = nn.LayerNorm(normalized_shape = (N_EMBD, ))\n",
        "    self.mlp = MLP()\n",
        "\n",
        "  def forward(self, x, cos, sin):\n",
        "    x = x + self.sa(self.ln_1(x), cos, sin)\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(VOCAB_SIZE, N_EMBD)\n",
        "        self.h = nn.ModuleList([Block() for _ in range(N_LAYERS)])\n",
        "        self.ln_f = nn.LayerNorm(N_EMBD)\n",
        "        self.lm_head = nn.Linear(N_EMBD, VOCAB_SIZE)\n",
        "\n",
        "        cos, sin = self._precompute_rotary_embeddings(BLOCK_SIZE)\n",
        "        self.register_buffer(\"cos\", cos, persistent=False)\n",
        "        self.register_buffer(\"sin\", sin, persistent=False)\n",
        "\n",
        "\n",
        "    def _precompute_rotary_embeddings(self, seq_len, base=10000):\n",
        "        head_dim = N_EMBD // N_HEAD\n",
        "        channel_range = torch.arange(0, head_dim, 2, device=device) # (head_dim / 2, )\n",
        "        inv_freq = 1 / (base ** (channel_range / head_dim)) # (head_dim / 2, )\n",
        "        t = torch.arange(seq_len, device=device) # (seq_len, )\n",
        "\n",
        "        freqs = torch.outer(t, inv_freq) # (seq_len, head_dim / 2)\n",
        "        cos, sin = freqs.cos().bfloat16(), freqs.sin().bfloat16()\n",
        "        cos, sin = cos[None, :, None, :], sin[None, :, None, :] # (1, seq_len, 1, head_dim/2)\n",
        "        return cos, sin\n",
        "\n",
        "\n",
        "    def forward(self, inp, labels=None, ignore_index=-1):\n",
        "        x = self.tok_emb(inp) # (B,T) --> (B,T,C)\n",
        "        T = x.size(1)\n",
        "        for layer_idx in range(N_LAYERS):\n",
        "          x = self.h[layer_idx](x, self.cos[:, :T, :, :], self.sin[:, :T, :, :]) # (B,T,C) --> (B,T,C)\n",
        "        logits = self.lm_head(self.ln_f(x))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "          # labels\n",
        "          # logits shape: (B, T, V)\n",
        "          # labels.shape: (B, T)\n",
        "          loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=ignore_index)\n",
        "        return logits, loss\n",
        "\n",
        "######################################\n",
        "# sampling methods\n",
        "\n",
        "def pick_next_greedy(model, inp):\n",
        "    if inp.device != device:\n",
        "        inp = inp.to(device)\n",
        "\n",
        "    logits, _ = model(inp) # logits will have shape: (B, T, V)\n",
        "    last_tok_logits = logits[:, -1, :] # (B, V)\n",
        "\n",
        "    _, top_idx = torch.max(last_tok_logits, dim=1, keepdim=True)\n",
        "    return top_idx\n",
        "\n",
        "\n",
        "\n",
        "def pick_next_top_p(model, inp, p):\n",
        "    \"\"\"\n",
        "    model chooses from the smallest possible set of tokens whose cumulative probability mass exceeds p\n",
        "    \"\"\"\n",
        "    if inp.device != device:\n",
        "        inp = inp.to(device)\n",
        "\n",
        "    logits, _ = model(inp) # logits will have shape: (B, T, V)\n",
        "    last_tok_logits = logits[:, -1, :] # (B, V)\n",
        "    last_tok_probs = torch.softmax(last_tok_logits, dim=1) # (B, V)\n",
        "    sorted_probs, sorted_indices = torch.sort(last_tok_probs, dim=-1, descending=True)\n",
        "\n",
        "    # say p = 0.75\n",
        "    # 0.5, 0.20,   0.2,  0.05,  0.05\n",
        "    # 0.5, 0.70,  0.90,  0.95,  1.00\n",
        "    # 0,   0.5   0.7,\n",
        "\n",
        "    cumsum = torch.cumsum(sorted_probs, dim=1)\n",
        "    masked_probs = torch.where(cumsum - sorted_probs <= p, sorted_probs, 0) # (B, V)\n",
        "    masked_probs = masked_probs / torch.sum(masked_probs, dim=1, keepdim=True)\n",
        "\n",
        "    indices = torch.multinomial(masked_probs, 1) # (B, 1)\n",
        "    actual_indices = torch.gather(sorted_indices, 1, indices)\n",
        "    return actual_indices\n",
        "\n",
        "\n",
        "def pick_next_top_k(model, inp, k):\n",
        "    if inp.device != device:\n",
        "        inp = inp.to(device)\n",
        "\n",
        "    logits, _ = model(inp) # logits will have shape: (B, T, V)\n",
        "    last_tok_logits = logits[:, -1, :] # (B, V)\n",
        "    topk_vals, _ = torch.topk(last_tok_logits, k=k, dim=1) # (B, k)\n",
        "    kth_val = topk_vals[:, [-1]] # (B, 1)\n",
        "\n",
        "    last_tok_logits = torch.where(last_tok_logits >= kth_val, last_tok_logits, -float('inf')) # (B, V)\n",
        "    last_tok_probs = torch.softmax(last_tok_logits, dim=1) # (B, V)\n",
        "\n",
        "    next_tokens = torch.multinomial(last_tok_probs, 1) # (B, 1)\n",
        "    return next_tokens\n",
        "\n",
        "\n",
        "def generate(model, start_prompt, top_p=None, top_k=None, num_tokens=50, num_samples=16):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start_tokens = []\n",
        "        # tokenize the start prompt\n",
        "        for c in start_prompt:\n",
        "            if c not in stoi:\n",
        "                raise ValueError(f\"{c} was found in start_prompt for generate, but is not present in vocabulary\")\n",
        "            start_tokens.append(stoi[c])\n",
        "\n",
        "        inp = torch.tensor(start_tokens, dtype=torch.long)\n",
        "        inp = inp.to(next(model.parameters()).device)\n",
        "        inp = inp.view(1, -1).expand(num_samples, inp.size(0))\n",
        "\n",
        "        for i in range(num_tokens):\n",
        "            if inp.size(1) > BLOCK_SIZE:\n",
        "                x = inp[:, -BLOCK_SIZE:]\n",
        "            else:\n",
        "                x = inp\n",
        "\n",
        "            if top_p:\n",
        "                next_tokens = pick_next_top_p(model, x, top_p)\n",
        "            elif top_k:\n",
        "                next_tokens = pick_next_top_k(model, x, top_k)\n",
        "            else:\n",
        "                next_tokens = pick_next_greedy(model, x)\n",
        "\n",
        "            # concatenate with inp\n",
        "            inp = torch.cat([inp, next_tokens], dim=1)\n",
        "\n",
        "        # move back to cpu, decode back to strings and return\n",
        "        generations = []\n",
        "        inp = inp.cpu()\n",
        "        for i in range(len(inp)):\n",
        "            curr_generation = \"\".join([itos[i] for i in inp[i, :].tolist()])\n",
        "            generations.append(curr_generation)\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "    return generations\n",
        "\n",
        "##########################################\n",
        "\n",
        "\n",
        "def main():\n",
        "    # training time\n",
        "    TRAIN_BATCH_SIZE = 64\n",
        "    EVAL_BATCH_SIZE = 128\n",
        "    learning_rate = 1e-4\n",
        "\n",
        "    num_epochs = 1\n",
        "    eval_every = 500\n",
        "    print_every = 100\n",
        "    sample_prefix = \"WHEREFORE ARE THOU ROMEO\"\n",
        "    sample_every = 500\n",
        "    num_sample_tokens = 200\n",
        "    top_k = 13; top_p = None\n",
        "\n",
        "    # define datasets\n",
        "    train_dataset = ShakespeareDataset(split='train')\n",
        "    val_dataset = ShakespeareDataset(split='val')\n",
        "    # test_dataset = ShakespeareDataset(split='test')\n",
        "\n",
        "    # define dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size = EVAL_BATCH_SIZE, shuffle=False)\n",
        "    # test_dataloader = DataLoader(test_dataset, batch_size = EVAL_BATCH_SIZE, shuffle=False)\n",
        "    def run_eval():\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        num_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_dataloader:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                _, loss = model(x, labels=y)\n",
        "                total_loss += loss.item() * len(x)\n",
        "                num_samples +=  len(x)\n",
        "        model.train() # put back into training mode\n",
        "        return total_loss / num_samples\n",
        "\n",
        "\n",
        "\n",
        "    # define model and optimzier\n",
        "    model = GPT()\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "\n",
        "    run_name = \"exp_4\"\n",
        "    wandb.init(\n",
        "        project=\"shakespeare-gpt\",\n",
        "        name=run_name,\n",
        "        config={\n",
        "            \"block_size\": BLOCK_SIZE,\n",
        "            \"n_embd\": N_EMBD,\n",
        "            \"n_layers\": N_LAYERS,\n",
        "            \"n_head\": N_HEAD,\n",
        "            \"dropout\": DROPOUT_PROB,\n",
        "            \"use_bias\": USE_BIAS,\n",
        "            \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
        "            \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"num_epochs\": num_epochs,\n",
        "            \"eval_every\": eval_every,\n",
        "            \"print_every\": print_every,\n",
        "        },\n",
        "    )\n",
        "    wandb.watch(model, log=\"gradients\", log_freq=print_every)\n",
        "\n",
        "    iter_idx = 0\n",
        "    total_num_iter = num_epochs * len(train_dataloader)\n",
        "    # core training loop\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        for x,y in train_dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            _, loss = model(x, labels=y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iter_idx % print_every == 0:\n",
        "                train_loss = loss.item()\n",
        "                print(f\"Iter - {iter_idx}/{total_num_iter}, Train Loss: {train_loss}\")\n",
        "                wandb.log({\"train/loss\": train_loss, \"epoch\": epoch, \"iter\": iter_idx}, step=iter_idx)\n",
        "\n",
        "            if iter_idx % eval_every == 0:\n",
        "                eval_loss = run_eval()\n",
        "                print(f\"Iter - {iter_idx}/{total_num_iter}, Val Loss: {eval_loss}\")\n",
        "                wandb.log({\"val/loss\": eval_loss, \"epoch\": epoch, \"iter\": iter_idx}, step=iter_idx)\n",
        "\n",
        "            if iter_idx % sample_every == 0:\n",
        "                generations = generate(\n",
        "                    model,\n",
        "                    start_prompt=sample_prefix,\n",
        "                    top_p=top_p,\n",
        "                    top_k=top_k,\n",
        "                    num_tokens=num_sample_tokens,\n",
        "                    num_samples=1,\n",
        "                )\n",
        "                if generations:\n",
        "                    sample_text = generations[0]\n",
        "                    print(f\"Iter - {iter_idx}/{total_num_iter}, Sample Generation:\\n{sample_text}\")\n",
        "                    wandb.log({\"sample/text\": sample_text, \"epoch\": epoch, \"iter\": iter_idx}, step=iter_idx)\n",
        "\n",
        "\n",
        "            iter_idx += 1\n",
        "\n",
        "\n",
        "    # save the model state at the end so we can evaluate later (we don't really need the optimizer, but just in case)\n",
        "    model_path = SCRIPT_DIR / \"model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    wandb.save(str(model_path))\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boDvim6EaPhX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}